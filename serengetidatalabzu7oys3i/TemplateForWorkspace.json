{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "serengetidatalabzu7oys3i"
		},
		"serengetidatalabzu7oys3i-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'serengetidatalabzu7oys3i-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:serengetidatalabzu7oys3i.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureKeyVault_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://serengetikeyvaultzu7oys3.vault.azure.net/"
		},
		"serengetidatalabzu7oys3i-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://serengetistorezu7oys3io6.dfs.core.windows.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/serengetidatalabzu7oys3i-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('serengetidatalabzu7oys3i-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/serengetidatalabzu7oys3i-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('serengetidatalabzu7oys3i-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GenerateData_Relational')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ApacheSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6fd8b876-6adc-456a-9952-b317ae5eb4b5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/2aa92634-96f9-45f8-a0aa-a3789b73ce2d/resourceGroups/rgp-salvo-sbx/providers/Microsoft.Synapse/workspaces/serengetidatalabzu7oys3i/bigDataPools/ApacheSpark",
						"name": "ApacheSpark",
						"type": "Spark",
						"endpoint": "https://serengetidatalabzu7oys3i.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run PromptFunction_Relational"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.core.platform import find_secret\n",
							"\n",
							"# Fill in the following lines with your service information\n",
							"service_name = \"echostorai\" # Name of your OpenAI service\n",
							"deployment_name = \"text-davinci-003\" # Name of your deployment in OpenAI\n",
							"key = \"4d263a830b0d499b9096258cd2124a65\"  # replace this with your secret and keyvault\n",
							"\n",
							"# Note: a cartesian product of customers and restaurants will be generated\n",
							"NrOfCustomers = 10 # Set number of Customers\n",
							"NrOfAccounts =  10 # Set number of accounts\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfcustomerids = spark.range(1,NrOfCustomers + 1) \\\n",
							"    .withColumnRenamed(\"id\", \"customerid\") \\\n",
							"    .withColumn(\"prompt\", customer_prompt_udf())\n",
							"\n",
							"display(dfcustomerids)\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfaccountids = spark.range(1,NrOfAccounts + 1) \\\n",
							"    .withColumnRenamed(\"id\", \"accountid\") \\\n",
							"    .withColumn(\"prompt\", account_prompt_udf())\n",
							"\n",
							"display(dfaccountids)\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.cognitive import OpenAICompletion\n",
							"\n",
							"OpenAICompletion = (\n",
							"    OpenAICompletion()\n",
							"    .setSubscriptionKey(key)\n",
							"    .setDeploymentName(deployment_name)\n",
							"    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
							"    .setMaxTokens(2048)\n",
							"    .setPromptCol(\"prompt\")\n",
							"    .setErrorCol(\"error\")\n",
							"    .setOutputCol(\"response\")\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import col\n",
							"\n",
							"df_customerobject = OpenAICompletion.transform(dfcustomerids) \\\n",
							"    .select(col('customerid'), col('response.choices.text').getItem(0).alias('customerobject'))\\\n",
							"    .cache()\n",
							"\n",
							"df_accountobject = OpenAICompletion.transform(dfaccountids) \\\n",
							"    .select(col('accountid'), col('response.choices.text').getItem(0).alias('accountobject'))\\\n",
							"    .cache()\n",
							"\n",
							"display(df_customerobject)\n",
							"display(df_accountobject)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
							"from pyspark.sql.functions import col, from_json\n",
							"\n",
							"schemaCustomer = StructType([ \\\n",
							"        StructField(\"firstname\", StringType(), False), \\\n",
							"        StructField(\"lastname\", StringType(), False), \\\n",
							"        StructField(\"accountid\", StringType(), False), \\\n",
							"        StructField(\"city\", StringType(), False), \\\n",
							"        StructField(\"state\", StringType(), False), \\\n",
							"        StructField(\"zipcode\", StringType(), False) \\\n",
							"        ])\n",
							"\n",
							"schemaAccount = StructType([ \\\n",
							"        StructField(\"accountid\", StringType(), False), \\\n",
							"        StructField(\"balance\", DoubleType(), False) \\\n",
							"        ])\n",
							"\n",
							"\n",
							"\n",
							"df_customer = df_customerobject.withColumn(\"json\",from_json(col(\"customerobject\"), schemaCustomer))\\\n",
							"    .select(col(\"customerID\"), col(\"json.*\"))\n",
							"\n",
							"df_account = df_accountobject.withColumn(\"json\",from_json(col(\"accountobject\"), schemaAccount))\\\n",
							"    .select(col(\"accountid\"), col(\"json.*\"))\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"display(df_customer)\n",
							"display(df_accountobject)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import col, row_number, window, rand\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
							"\n",
							"\n",
							"cross_joined_df = df_customer.crossJoin(df_restaurant) \\\n",
							"    .withColumn(\"prompt\", reviews_prompt_udf(col(\"restaurant\"), ((5 * rand()).cast(\"int\") + 1))) \\\n",
							"\n",
							"df_reviewobject = OpenAICompletion.transform(cross_joined_df) \\\n",
							"    .withColumn(\"reviewObject\",col('response.choices.text').getItem(0))\\\n",
							"    .select(col(\"customerid\"),col(\"restaurantid\"), col(\"reviewobject\")) \n",
							"\n",
							"\n",
							"schemaReview = StructType([ \\\n",
							"        StructField(\"reviewdate\", StringType(), False), \\\n",
							"        StructField(\"review\", StringType(), False), \\\n",
							"        StructField(\"rating\", IntegerType(), False) \\\n",
							"        ])\n",
							"\n",
							"\n",
							"df_review = df_reviewobject.withColumn(\"json\",from_json(col(\"reviewobject\"), schemaReview))\\\n",
							"    .select(col(\"restaurantid\"), col(\"customerid\"), col(\"json.*\"))\n",
							"\n",
							"display(df_review)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PromptFunction_Relational')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ApacheSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9fb09333-ade0-4db5-8dc2-608fd1873dc8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/2aa92634-96f9-45f8-a0aa-a3789b73ce2d/resourceGroups/rgp-salvo-sbx/providers/Microsoft.Synapse/workspaces/serengetidatalabzu7oys3i/bigDataPools/ApacheSpark",
						"name": "ApacheSpark",
						"type": "Spark",
						"endpoint": "https://serengetidatalabzu7oys3i.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ApacheSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"Create UDFs 'tbl_prompt()) -> str'\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql.functions import udf\n",
							"from pyspark.sql.types import StringType, IntegerType\n",
							"\n",
							"@udf(returnType=StringType())\n",
							"def customer_prompt_udf():\n",
							"    return \"Generate a json containing banking customer information. The accountid field needs to be unique. Use the following json structure: \\\n",
							"{ \\\n",
							"        \\\"firstname\\\": \\\"\\\", \\\n",
							"        \\\"lastname\\\": \\\"\\\", \\\n",
							"        \\\"accountid\\\": \\\"\\\", \\\n",
							"        \\\"city\\\": \\\"\\\", \\\n",
							"        \\\"state\\\": \\\"\\\", \\\n",
							"        \\\"zipcode\\\": \\\"\\\" \\\n",
							"}\"\n",
							"\n",
							"@udf(returnType=StringType())\n",
							"def account_prompt_udf():\n",
							"    return \"Generate a json containing banking account infromation. Use the following json structure:: \\\n",
							"{ \\\n",
							"        \\\"accountid\\\": \\\"\\\", \\\n",
							"        \\\"balance\\\": \\\"\\\" \\\n",
							"}\"\n",
							"\n",
							"@udf(returnType=StringType())\n",
							"def transaction_prompt_udf(accountid: str) -> str:    \n",
							"    return \"Create a json containing a bank transaction for accountid \"+ accountid + \" Use the following json structure: \\\n",
							"{ \\\n",
							"    \\\"transaction date\\\": \\\"yyyy-mm-dd\\\", \\\n",
							"    \\\"accountid\\\": \\\"\\\", \\\n",
							"    \\\"amount\\\": \\\"\\\" \\\n",
							"} \"\n",
							"\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"UDFs created\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ApacheSpark')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}